name: Scrape Data Every 7 Hours

on:
  schedule:
    - cron: '0 */7 * * *' # Runs every 7 hours
  workflow_dispatch: # Allows manual triggering

jobs:

  scrape:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    - name: Set up Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '20' # Using Node.js version 20

    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.npm
        key: ${{ runner.OS }}-node-${{ hashFiles('**/package-lock.json') }}
        restore-keys: |
          ${{ runner.OS }}-node-

    - name: Install dependencies
      run: npm ci

    - name: Get chmod
      run: chmod +x backend/scrape.mjs

    - name: Start API server
      run: |
        nohup node backend/api.js & # Starts the API server in the background
        sleep 60 # Wait for the server to start

    - name: Run scraper script
      run: node backend/scrape.mjs # Ensure this script handles scraping and storing the data

    - name: Commit and push changes
      run: |
        git config --global user.name "github-actions[bot]"
        git config --global user.email "41898282+github-actions[bot]@users.noreply.github.com"
        git add .
        git diff --quiet && git diff --staged --quiet || (git commit -m "Auto-update data file" && git push)
